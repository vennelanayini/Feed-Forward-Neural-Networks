{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMb/QxGIuIzci+KROvbkIr+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vennelanayini/Feed-Forward-Neural-Networks/blob/main/160122737166-DL_Assignment_1(MNIST).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "# Fetch MNIST dataset\n",
        "data, labels = fetch_openml('mnist_784', version=1, return_X_y=True, parser='pandas')\n",
        "data = data.to_numpy() / 255.0  # Normalize pixel values between 0 and 1\n",
        "labels = labels.to_numpy().astype(int)\n",
        "# Convert labels to one-hot encoding\n",
        "encoder = OneHotEncoder(sparse_output=False, categories='auto')\n",
        "y_encoded = encoder.fit_transform(labels.reshape(-1, 1))\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data, y_encoded, test_size=0.10, random_state=42, stratify=y_encoded\n",
        ")\n",
        "# Neural network structure\n",
        "input_nodes = 784   # 28x28 images\n",
        "hidden_nodes = 50   # Hidden layer size\n",
        "output_nodes = 10   # 10 digit classes\n",
        "learning_step = 0.01\n",
        "epochs = 10  # Adjusted to range from 5 to 10\n",
        "batch_count = 32\n",
        "optimization_methods = ['sgd', 'momentum', 'nesterov', 'rmsprop', 'adam']\n",
        "# Initialize parameters\n",
        "np.random.seed(42)\n",
        "W1 = np.random.randn(input_nodes, hidden_nodes) * 0.01\n",
        "b1 = np.zeros((1, hidden_nodes))\n",
        "W2 = np.random.randn(hidden_nodes, output_nodes) * 0.01\n",
        "b2 = np.zeros((1, output_nodes))\n",
        "# Activation functions\n",
        "def activation_sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "def activation_softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "def loss_cross_entropy(predictions, actual):\n",
        "    return -np.mean(np.sum(actual * np.log(predictions + 1e-9), axis=1))\n",
        "def evaluate_accuracy(predictions, actual):\n",
        "    return np.mean(np.argmax(predictions, axis=1) == np.argmax(actual, axis=1))\n",
        "for method in optimization_methods:\n",
        "    print(f\"Training using {method} optimizer\")\n",
        "    # Initialize weights again\n",
        "    W1 = np.random.randn(input_nodes, hidden_nodes) * 0.01\n",
        "    b1 = np.zeros((1, hidden_nodes))\n",
        "    W2 = np.random.randn(hidden_nodes, output_nodes) * 0.01\n",
        "    b2 = np.zeros((1, output_nodes))\n",
        "    velocity_W1 = np.zeros_like(W1)\n",
        "    velocity_b1 = np.zeros_like(b1)\n",
        "    velocity_W2 = np.zeros_like(W2)\n",
        "    velocity_b2 = np.zeros_like(b2)\n",
        "    for cycle in range(5, epochs + 1):  # Starts at 5\n",
        "        # Forward propagation\n",
        "        layer1_output = np.dot(X_train, W1) + b1\n",
        "        activated_layer1 = activation_sigmoid(layer1_output)\n",
        "        layer2_output = np.dot(activated_layer1, W2) + b2\n",
        "        predictions = activation_softmax(layer2_output)\n",
        "        # Compute loss and accuracy\n",
        "        loss_value = loss_cross_entropy(predictions, y_train)\n",
        "        accuracy_value = evaluate_accuracy(predictions, y_train)\n",
        "        # Backpropagation process\n",
        "        error_output = predictions - y_train  # Output layer gradient\n",
        "        gradient_W2 = np.dot(activated_layer1.T, error_output) / len(X_train)\n",
        "        gradient_b2 = np.sum(error_output, axis=0, keepdims=True) / len(X_train)\n",
        "        error_hidden = np.dot(error_output, W2.T)\n",
        "        gradient_hidden = error_hidden * activated_layer1 * (1 - activated_layer1)\n",
        "        gradient_W1 = np.dot(X_train.T, gradient_hidden) / len(X_train)\n",
        "        gradient_b1 = np.sum(gradient_hidden, axis=0, keepdims=True) / len(X_train)\n",
        "        # Apply optimization techniques\n",
        "        if method == 'sgd':\n",
        "            W1 -= learning_step * gradient_W1\n",
        "            b1 -= learning_step * gradient_b1\n",
        "            W2 -= learning_step * gradient_W2\n",
        "            b2 -= learning_step * gradient_b2\n",
        "        elif method == 'momentum':\n",
        "            momentum_factor = 0.9\n",
        "            velocity_W1 = momentum_factor * velocity_W1 - learning_step * gradient_W1\n",
        "            velocity_b1 = momentum_factor * velocity_b1 - learning_step * gradient_b1\n",
        "            velocity_W2 = momentum_factor * velocity_W2 - learning_step * gradient_W2\n",
        "            velocity_b2 = momentum_factor * velocity_b2 - learning_step * gradient_b2\n",
        "            W1 += velocity_W1\n",
        "            b1 += velocity_b1\n",
        "            W2 += velocity_W2\n",
        "            b2 += velocity_b2\n",
        "        elif method == 'adam':\n",
        "            beta1, beta2, epsilon = 0.9, 0.999, 1e-8\n",
        "            m_W1, v_W1 = np.zeros_like(W1), np.zeros_like(W1)\n",
        "            m_b1, v_b1 = np.zeros_like(b1), np.zeros_like(b1)\n",
        "            m_W2, v_W2 = np.zeros_like(W2), np.zeros_like(W2)\n",
        "            m_b2, v_b2 = np.zeros_like(b2), np.zeros_like(b2)\n",
        "            m_W1 = beta1 * m_W1 + (1 - beta1) * gradient_W1\n",
        "            v_W1 = beta2 * v_W1 + (1 - beta2) * (gradient_W1 ** 2)\n",
        "            W1 -= learning_step * m_W1 / (np.sqrt(v_W1) + epsilon)\n",
        "            m_b1 = beta1 * m_b1 + (1 - beta1) * gradient_b1\n",
        "            v_b1 = beta2 * v_b1 + (1 - beta2) * (gradient_b1 ** 2)\n",
        "            b1 -= learning_step * m_b1 / (np.sqrt(v_b1) + epsilon)\n",
        "            m_W2 = beta1 * m_W2 + (1 - beta1) * gradient_W2\n",
        "            v_W2 = beta2 * v_W2 + (1 - beta2) * (gradient_W2 ** 2)\n",
        "            W2 -= learning_step * m_W2 / (np.sqrt(v_W2) + epsilon)\n",
        "            m_b2 = beta1 * m_b2 + (1 - beta1) * gradient_b2\n",
        "            v_b2 = beta2 * v_b2 + (1 - beta2) * (gradient_b2 ** 2)\n",
        "            b2 -= learning_step * m_b2 / (np.sqrt(v_b2) + epsilon)\n",
        "        print(f\"Cycle {cycle}/{epochs} - Loss: {loss_value:.4f} - Accuracy: {accuracy_value:.4f}\")\n",
        "    # Evaluate performance on test data\n",
        "    test_layer1 = np.dot(X_test, W1) + b1\n",
        "    test_activated1 = activation_sigmoid(test_layer1)\n",
        "    test_layer2 = np.dot(test_activated1, W2) + b2\n",
        "    test_predictions = activation_softmax(test_layer2)\n",
        "    final_accuracy = evaluate_accuracy(test_predictions, y_test)\n",
        "    print(f\"Final Accuracy with {method}: {final_accuracy:.4f}\\n\")\n",
        "\n",
        "'''Optimizer: SGD\n",
        "Learning Rate: 0.1\n",
        "Hidden Layer Size: 64\n",
        "Reason:\n",
        "Stochastic Gradient Descent (SGD) is a simple and effective optimizer.\n",
        "A learning rate of 0.1 ensures the model converges faster than a very small learning rate.\n",
        "Increasing hidden neurons from 50 to 64 allows for better feature representation.\n",
        "Expected Accuracy: ~92%\n",
        "Configuration 2 (Adam Optimizer with Tuned Parameters)\n",
        "\n",
        "Optimizer: Adam\n",
        "Learning Rate: 0.001\n",
        "Hidden Layer Size: 128\n",
        "Reason:\n",
        "Adam is known to work well for image classification problems, adjusting learning rates adaptively.\n",
        "A higher hidden layer size (128 neurons) improves model capacity.\n",
        "Lower learning rate (0.001) prevents instability and overfitting.\n",
        "Expected Accuracy: ~97%\n",
        "Configuration 3 (Momentum-Accelerated Learning)\n",
        "\n",
        "Optimizer: Momentum (Î²=0.9)\n",
        "Learning Rate: 0.01\n",
        "Hidden Layer Size: 100\n",
        "Reason:\n",
        "Momentum helps in faster convergence by keeping past gradients in memory.\n",
        "Learning rate of 0.01 ensures smooth updates without overshooting.\n",
        "100 hidden neurons provide a balance between complexity and generalization\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGnREkF3mN7r",
        "outputId": "64788a25-1909-4351-f060-7cb691eba1f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training using sgd optimizer\n",
            "Cycle 5/10 - Loss: 2.3040 - Accuracy: 0.0994\n",
            "Cycle 6/10 - Loss: 2.3039 - Accuracy: 0.0994\n",
            "Cycle 7/10 - Loss: 2.3038 - Accuracy: 0.0994\n",
            "Cycle 8/10 - Loss: 2.3037 - Accuracy: 0.0994\n",
            "Cycle 9/10 - Loss: 2.3037 - Accuracy: 0.0994\n",
            "Cycle 10/10 - Loss: 2.3036 - Accuracy: 0.0994\n",
            "Final Accuracy with sgd: 0.0994\n",
            "\n",
            "Training using momentum optimizer\n",
            "Cycle 5/10 - Loss: 2.3040 - Accuracy: 0.0994\n",
            "Cycle 6/10 - Loss: 2.3039 - Accuracy: 0.0994\n",
            "Cycle 7/10 - Loss: 2.3037 - Accuracy: 0.0994\n",
            "Cycle 8/10 - Loss: 2.3035 - Accuracy: 0.0994\n",
            "Cycle 9/10 - Loss: 2.3032 - Accuracy: 0.0994\n",
            "Cycle 10/10 - Loss: 2.3029 - Accuracy: 0.0994\n",
            "Final Accuracy with momentum: 0.0994\n",
            "\n",
            "Training using nesterov optimizer\n",
            "Cycle 5/10 - Loss: 2.3039 - Accuracy: 0.0985\n",
            "Cycle 6/10 - Loss: 2.3039 - Accuracy: 0.0985\n",
            "Cycle 7/10 - Loss: 2.3039 - Accuracy: 0.0985\n",
            "Cycle 8/10 - Loss: 2.3039 - Accuracy: 0.0985\n",
            "Cycle 9/10 - Loss: 2.3039 - Accuracy: 0.0985\n",
            "Cycle 10/10 - Loss: 2.3039 - Accuracy: 0.0985\n",
            "Final Accuracy with nesterov: 0.0984\n",
            "\n",
            "Training using rmsprop optimizer\n",
            "Cycle 5/10 - Loss: 2.3023 - Accuracy: 0.1020\n",
            "Cycle 6/10 - Loss: 2.3023 - Accuracy: 0.1020\n",
            "Cycle 7/10 - Loss: 2.3023 - Accuracy: 0.1020\n",
            "Cycle 8/10 - Loss: 2.3023 - Accuracy: 0.1020\n",
            "Cycle 9/10 - Loss: 2.3023 - Accuracy: 0.1020\n",
            "Cycle 10/10 - Loss: 2.3023 - Accuracy: 0.1020\n",
            "Final Accuracy with rmsprop: 0.1020\n",
            "\n",
            "Training using adam optimizer\n",
            "Cycle 5/10 - Loss: 2.3035 - Accuracy: 0.0977\n",
            "Cycle 6/10 - Loss: 2.4676 - Accuracy: 0.2888\n",
            "Cycle 7/10 - Loss: 2.2813 - Accuracy: 0.2461\n",
            "Cycle 8/10 - Loss: 2.0542 - Accuracy: 0.2538\n",
            "Cycle 9/10 - Loss: 1.7835 - Accuracy: 0.5840\n",
            "Cycle 10/10 - Loss: 1.5518 - Accuracy: 0.6380\n",
            "Final Accuracy with adam: 0.5859\n",
            "\n"
          ]
        }
      ]
    }
  ]
}