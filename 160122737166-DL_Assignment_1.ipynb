{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNMuliQR90AnZOI9dxdC4TG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vennelanayini/Feed-Forward-Neural-Networks/blob/main/160122737166-DL_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "# Fetch MNIST dataset\n",
        "data, labels = fetch_openml('mnist_784', version=1, return_X_y=True, parser='pandas')\n",
        "data = data.to_numpy() / 255.0  # Normalize pixel values between 0 and 1\n",
        "labels = labels.to_numpy().astype(int)\n",
        "# Convert labels to one-hot encoding\n",
        "encoder = OneHotEncoder(sparse_output=False, categories='auto')\n",
        "y_encoded = encoder.fit_transform(labels.reshape(-1, 1))\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data, y_encoded, test_size=0.10, random_state=42, stratify=y_encoded\n",
        ")\n",
        "# Neural network structure\n",
        "input_nodes = 784   # 28x28 images\n",
        "hidden_nodes = 50   # Hidden layer size\n",
        "output_nodes = 10   # 10 digit classes\n",
        "learning_step = 0.01\n",
        "epochs = 10  # Adjusted to range from 5 to 10\n",
        "batch_count = 32\n",
        "optimization_methods = ['sgd', 'momentum', 'nesterov', 'rmsprop', 'adam']\n",
        "# Initialize parameters\n",
        "np.random.seed(42)\n",
        "W1 = np.random.randn(input_nodes, hidden_nodes) * 0.01\n",
        "b1 = np.zeros((1, hidden_nodes))\n",
        "W2 = np.random.randn(hidden_nodes, output_nodes) * 0.01\n",
        "b2 = np.zeros((1, output_nodes))\n",
        "# Activation functions\n",
        "def activation_sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "def activation_softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "def loss_cross_entropy(predictions, actual):\n",
        "    return -np.mean(np.sum(actual * np.log(predictions + 1e-9), axis=1))\n",
        "def evaluate_accuracy(predictions, actual):\n",
        "    return np.mean(np.argmax(predictions, axis=1) == np.argmax(actual, axis=1))\n",
        "for method in optimization_methods:\n",
        "    print(f\"Training using {method} optimizer\")\n",
        "    # Initialize weights again\n",
        "    W1 = np.random.randn(input_nodes, hidden_nodes) * 0.01\n",
        "    b1 = np.zeros((1, hidden_nodes))\n",
        "    W2 = np.random.randn(hidden_nodes, output_nodes) * 0.01\n",
        "    b2 = np.zeros((1, output_nodes))\n",
        "    velocity_W1 = np.zeros_like(W1)\n",
        "    velocity_b1 = np.zeros_like(b1)\n",
        "    velocity_W2 = np.zeros_like(W2)\n",
        "    velocity_b2 = np.zeros_like(b2)\n",
        "    for cycle in range(5, epochs + 1):  # Starts at 5\n",
        "        # Forward propagation\n",
        "        layer1_output = np.dot(X_train, W1) + b1\n",
        "        activated_layer1 = activation_sigmoid(layer1_output)\n",
        "        layer2_output = np.dot(activated_layer1, W2) + b2\n",
        "        predictions = activation_softmax(layer2_output)\n",
        "        # Compute loss and accuracy\n",
        "        loss_value = loss_cross_entropy(predictions, y_train)\n",
        "        accuracy_value = evaluate_accuracy(predictions, y_train)\n",
        "        # Backpropagation process\n",
        "        error_output = predictions - y_train  # Output layer gradient\n",
        "        gradient_W2 = np.dot(activated_layer1.T, error_output) / len(X_train)\n",
        "        gradient_b2 = np.sum(error_output, axis=0, keepdims=True) / len(X_train)\n",
        "        error_hidden = np.dot(error_output, W2.T)\n",
        "        gradient_hidden = error_hidden * activated_layer1 * (1 - activated_layer1)\n",
        "        gradient_W1 = np.dot(X_train.T, gradient_hidden) / len(X_train)\n",
        "        gradient_b1 = np.sum(gradient_hidden, axis=0, keepdims=True) / len(X_train)\n",
        "        # Apply optimization techniques\n",
        "        if method == 'sgd':\n",
        "            W1 -= learning_step * gradient_W1\n",
        "            b1 -= learning_step * gradient_b1\n",
        "            W2 -= learning_step * gradient_W2\n",
        "            b2 -= learning_step * gradient_b2\n",
        "        elif method == 'momentum':\n",
        "            momentum_factor = 0.9\n",
        "            velocity_W1 = momentum_factor * velocity_W1 - learning_step * gradient_W1\n",
        "            velocity_b1 = momentum_factor * velocity_b1 - learning_step * gradient_b1\n",
        "            velocity_W2 = momentum_factor * velocity_W2 - learning_step * gradient_W2\n",
        "            velocity_b2 = momentum_factor * velocity_b2 - learning_step * gradient_b2\n",
        "            W1 += velocity_W1\n",
        "            b1 += velocity_b1\n",
        "            W2 += velocity_W2\n",
        "            b2 += velocity_b2\n",
        "        elif method == 'adam':\n",
        "            beta1, beta2, epsilon = 0.9, 0.999, 1e-8\n",
        "            m_W1, v_W1 = np.zeros_like(W1), np.zeros_like(W1)\n",
        "            m_b1, v_b1 = np.zeros_like(b1), np.zeros_like(b1)\n",
        "            m_W2, v_W2 = np.zeros_like(W2), np.zeros_like(W2)\n",
        "            m_b2, v_b2 = np.zeros_like(b2), np.zeros_like(b2)\n",
        "            m_W1 = beta1 * m_W1 + (1 - beta1) * gradient_W1\n",
        "            v_W1 = beta2 * v_W1 + (1 - beta2) * (gradient_W1 ** 2)\n",
        "            W1 -= learning_step * m_W1 / (np.sqrt(v_W1) + epsilon)\n",
        "            m_b1 = beta1 * m_b1 + (1 - beta1) * gradient_b1\n",
        "            v_b1 = beta2 * v_b1 + (1 - beta2) * (gradient_b1 ** 2)\n",
        "            b1 -= learning_step * m_b1 / (np.sqrt(v_b1) + epsilon)\n",
        "            m_W2 = beta1 * m_W2 + (1 - beta1) * gradient_W2\n",
        "            v_W2 = beta2 * v_W2 + (1 - beta2) * (gradient_W2 ** 2)\n",
        "            W2 -= learning_step * m_W2 / (np.sqrt(v_W2) + epsilon)\n",
        "            m_b2 = beta1 * m_b2 + (1 - beta1) * gradient_b2\n",
        "            v_b2 = beta2 * v_b2 + (1 - beta2) * (gradient_b2 ** 2)\n",
        "            b2 -= learning_step * m_b2 / (np.sqrt(v_b2) + epsilon)\n",
        "        print(f\"Cycle {cycle}/{epochs} - Loss: {loss_value:.4f} - Accuracy: {accuracy_value:.4f}\")\n",
        "    # Evaluate performance on test data\n",
        "    test_layer1 = np.dot(X_test, W1) + b1\n",
        "    test_activated1 = activation_sigmoid(test_layer1)\n",
        "    test_layer2 = np.dot(test_activated1, W2) + b2\n",
        "    test_predictions = activation_softmax(test_layer2)\n",
        "    final_accuracy = evaluate_accuracy(test_predictions, y_test)\n",
        "    print(f\"Final Accuracy with {method}: {final_accuracy:.4f}\\n\")\n",
        "\n",
        "'''Optimizer: SGD\n",
        "Learning Rate: 0.1\n",
        "Hidden Layer Size: 64\n",
        "Reason:\n",
        "Stochastic Gradient Descent (SGD) is a simple and effective optimizer.\n",
        "A learning rate of 0.1 ensures the model converges faster than a very small learning rate.\n",
        "Increasing hidden neurons from 50 to 64 allows for better feature representation.\n",
        "Expected Accuracy: ~92%\n",
        "Configuration 2 (Adam Optimizer with Tuned Parameters)\n",
        "\n",
        "Optimizer: Adam\n",
        "Learning Rate: 0.001\n",
        "Hidden Layer Size: 128\n",
        "Reason:\n",
        "Adam is known to work well for image classification problems, adjusting learning rates adaptively.\n",
        "A higher hidden layer size (128 neurons) improves model capacity.\n",
        "Lower learning rate (0.001) prevents instability and overfitting.\n",
        "Expected Accuracy: ~97%\n",
        "Configuration 3 (Momentum-Accelerated Learning)\n",
        "\n",
        "Optimizer: Momentum (β=0.9)\n",
        "Learning Rate: 0.01\n",
        "Hidden Layer Size: 100\n",
        "Reason:\n",
        "Momentum helps in faster convergence by keeping past gradients in memory.\n",
        "Learning rate of 0.01 ensures smooth updates without overshooting.\n",
        "100 hidden neurons provide a balance between complexity and generalization\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGnREkF3mN7r",
        "outputId": "64788a25-1909-4351-f060-7cb691eba1f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training using sgd optimizer\n",
            "Cycle 5/10 - Loss: 2.3040 - Accuracy: 0.0994\n",
            "Cycle 6/10 - Loss: 2.3039 - Accuracy: 0.0994\n",
            "Cycle 7/10 - Loss: 2.3038 - Accuracy: 0.0994\n",
            "Cycle 8/10 - Loss: 2.3037 - Accuracy: 0.0994\n",
            "Cycle 9/10 - Loss: 2.3037 - Accuracy: 0.0994\n",
            "Cycle 10/10 - Loss: 2.3036 - Accuracy: 0.0994\n",
            "Final Accuracy with sgd: 0.0994\n",
            "\n",
            "Training using momentum optimizer\n",
            "Cycle 5/10 - Loss: 2.3040 - Accuracy: 0.0994\n",
            "Cycle 6/10 - Loss: 2.3039 - Accuracy: 0.0994\n",
            "Cycle 7/10 - Loss: 2.3037 - Accuracy: 0.0994\n",
            "Cycle 8/10 - Loss: 2.3035 - Accuracy: 0.0994\n",
            "Cycle 9/10 - Loss: 2.3032 - Accuracy: 0.0994\n",
            "Cycle 10/10 - Loss: 2.3029 - Accuracy: 0.0994\n",
            "Final Accuracy with momentum: 0.0994\n",
            "\n",
            "Training using nesterov optimizer\n",
            "Cycle 5/10 - Loss: 2.3039 - Accuracy: 0.0985\n",
            "Cycle 6/10 - Loss: 2.3039 - Accuracy: 0.0985\n",
            "Cycle 7/10 - Loss: 2.3039 - Accuracy: 0.0985\n",
            "Cycle 8/10 - Loss: 2.3039 - Accuracy: 0.0985\n",
            "Cycle 9/10 - Loss: 2.3039 - Accuracy: 0.0985\n",
            "Cycle 10/10 - Loss: 2.3039 - Accuracy: 0.0985\n",
            "Final Accuracy with nesterov: 0.0984\n",
            "\n",
            "Training using rmsprop optimizer\n",
            "Cycle 5/10 - Loss: 2.3023 - Accuracy: 0.1020\n",
            "Cycle 6/10 - Loss: 2.3023 - Accuracy: 0.1020\n",
            "Cycle 7/10 - Loss: 2.3023 - Accuracy: 0.1020\n",
            "Cycle 8/10 - Loss: 2.3023 - Accuracy: 0.1020\n",
            "Cycle 9/10 - Loss: 2.3023 - Accuracy: 0.1020\n",
            "Cycle 10/10 - Loss: 2.3023 - Accuracy: 0.1020\n",
            "Final Accuracy with rmsprop: 0.1020\n",
            "\n",
            "Training using adam optimizer\n",
            "Cycle 5/10 - Loss: 2.3035 - Accuracy: 0.0977\n",
            "Cycle 6/10 - Loss: 2.4676 - Accuracy: 0.2888\n",
            "Cycle 7/10 - Loss: 2.2813 - Accuracy: 0.2461\n",
            "Cycle 8/10 - Loss: 2.0542 - Accuracy: 0.2538\n",
            "Cycle 9/10 - Loss: 1.7835 - Accuracy: 0.5840\n",
            "Cycle 10/10 - Loss: 1.5518 - Accuracy: 0.6380\n",
            "Final Accuracy with adam: 0.5859\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Fetch Fashion MNIST dataset\n",
        "data, labels = fetch_openml('Fashion-MNIST', version=1, return_X_y=True, parser='pandas')\n",
        "data = data.to_numpy() / 255.0  # Normalize pixel values between 0 and 1\n",
        "labels = labels.to_numpy().astype(int)\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "encoder = OneHotEncoder(sparse_output=False, categories='auto')\n",
        "y_encoded = encoder.fit_transform(labels.reshape(-1, 1))\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data, y_encoded, test_size=0.10, random_state=42, stratify=y_encoded\n",
        ")\n",
        "\n",
        "# Neural network structure\n",
        "input_nodes = 784   # 28x28 images\n",
        "hidden_layer_sizes = [64, 128, 100]  # Different configurations\n",
        "output_nodes = 10   # 10 clothing classes\n",
        "learning_rates = [0.1, 0.001, 0.01]\n",
        "optimizers = ['sgd', 'adam', 'momentum']\n",
        "epochs = 10\n",
        "\n",
        "# Activation functions\n",
        "def activation_sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def activation_softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "def loss_cross_entropy(predictions, actual):\n",
        "    return -np.mean(np.sum(actual * np.log(predictions + 1e-9), axis=1))\n",
        "\n",
        "def evaluate_accuracy(predictions, actual):\n",
        "    return np.mean(np.argmax(predictions, axis=1) == np.argmax(actual, axis=1))\n",
        "\n",
        "results = []\n",
        "\n",
        "for hidden_nodes, learning_rate, optimizer in zip(hidden_layer_sizes, learning_rates, optimizers):\n",
        "    print(f\"Training using {optimizer} optimizer with {hidden_nodes} hidden nodes and LR={learning_rate}\")\n",
        "\n",
        "    # Initialize weights\n",
        "    W1 = np.random.randn(input_nodes, hidden_nodes) * 0.01\n",
        "    b1 = np.zeros((1, hidden_nodes))\n",
        "    W2 = np.random.randn(hidden_nodes, output_nodes) * 0.01\n",
        "    b2 = np.zeros((1, output_nodes))\n",
        "\n",
        "    velocity_W1 = np.zeros_like(W1)\n",
        "    velocity_b1 = np.zeros_like(b1)\n",
        "    velocity_W2 = np.zeros_like(W2)\n",
        "    velocity_b2 = np.zeros_like(b2)\n",
        "    for cycle in range(1, epochs + 1):\n",
        "        # Forward propagation\n",
        "        layer1_output = np.dot(X_train, W1) + b1\n",
        "        activated_layer1 = activation_sigmoid(layer1_output)\n",
        "        layer2_output = np.dot(activated_layer1, W2) + b2\n",
        "        predictions = activation_softmax(layer2_output)\n",
        "        # Compute loss and accuracy\n",
        "        loss_value = loss_cross_entropy(predictions, y_train)\n",
        "        accuracy_value = evaluate_accuracy(predictions, y_train)\n",
        "        # Backpropagation process\n",
        "        error_output = predictions - y_train\n",
        "        gradient_W2 = np.dot(activated_layer1.T, error_output) / len(X_train)\n",
        "        gradient_b2 = np.sum(error_output, axis=0, keepdims=True) / len(X_train)\n",
        "        error_hidden = np.dot(error_output, W2.T)\n",
        "        gradient_hidden = error_hidden * activated_layer1 * (1 - activated_layer1)\n",
        "        gradient_W1 = np.dot(X_train.T, gradient_hidden) / len(X_train)\n",
        "        gradient_b1 = np.sum(gradient_hidden, axis=0, keepdims=True) / len(X_train)\n",
        "        # Apply optimization techniques\n",
        "        if optimizer == 'sgd':\n",
        "            W1 -= learning_rate * gradient_W1\n",
        "            b1 -= learning_rate * gradient_b1\n",
        "            W2 -= learning_rate * gradient_W2\n",
        "            b2 -= learning_rate * gradient_b2\n",
        "        elif optimizer == 'momentum':\n",
        "            momentum_factor = 0.9\n",
        "            velocity_W1 = momentum_factor * velocity_W1 - learning_rate * gradient_W1\n",
        "            velocity_b1 = momentum_factor * velocity_b1 - learning_rate * gradient_b1\n",
        "            velocity_W2 = momentum_factor * velocity_W2 - learning_rate * gradient_W2\n",
        "            velocity_b2 = momentum_factor * velocity_b2 - learning_rate * gradient_b2\n",
        "            W1 += velocity_W1\n",
        "            b1 += velocity_b1\n",
        "            W2 += velocity_W2\n",
        "            b2 += velocity_b2\n",
        "        elif optimizer == 'adam':\n",
        "            beta1, beta2, epsilon = 0.9, 0.999, 1e-8\n",
        "            m_W1, v_W1 = np.zeros_like(W1), np.zeros_like(W1)\n",
        "            m_b1, v_b1 = np.zeros_like(b1), np.zeros_like(b1)\n",
        "            m_W2, v_W2 = np.zeros_like(W2), np.zeros_like(W2)\n",
        "            m_b2, v_b2 = np.zeros_like(b2), np.zeros_like(b2)\n",
        "            m_W1 = beta1 * m_W1 + (1 - beta1) * gradient_W1\n",
        "            v_W1 = beta2 * v_W1 + (1 - beta2) * (gradient_W1 ** 2)\n",
        "            W1 -= learning_rate * m_W1 / (np.sqrt(v_W1) + epsilon)\n",
        "            m_b1 = beta1 * m_b1 + (1 - beta1) * gradient_b1\n",
        "            v_b1 = beta2 * v_b1 + (1 - beta2) * (gradient_b1 ** 2)\n",
        "            b1 -= learning_rate * m_b1 / (np.sqrt(v_b1) + epsilon)\n",
        "        print(f\"Cycle {cycle}/{epochs} - Loss: {loss_value:.4f} - Accuracy: {accuracy_value:.4f}\")\n",
        "    # Evaluate performance on test data\n",
        "    test_layer1 = np.dot(X_test, W1) + b1\n",
        "    test_activated1 = activation_sigmoid(test_layer1)\n",
        "    test_layer2 = np.dot(test_activated1, W2) + b2\n",
        "    test_predictions = activation_softmax(test_layer2)\n",
        "    final_accuracy = evaluate_accuracy(test_predictions, y_test)\n",
        "    print(f\"Final Accuracy with {optimizer}: {final_accuracy:.4f}\\n\")\n",
        "    results.append([optimizer, learning_rate, hidden_nodes, final_accuracy])\n",
        "# Display final results in tabular format\n",
        "results_df = pd.DataFrame(results, columns=[\"Optimizer\", \"Learning Rate\", \"Hidden Layer Size\", \"Final Accuracy\"])\n",
        "print(results_df)  # Prints the final conclusions in a table format"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPtISQLu2UOF",
        "outputId": "de0f8b85-0a72-4560-983d-c890ba10ef58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training using sgd optimizer with 64 hidden nodes and LR=0.1\n",
            "Cycle 1/10 - Loss: 2.3034 - Accuracy: 0.1000\n",
            "Cycle 2/10 - Loss: 2.3029 - Accuracy: 0.1000\n",
            "Cycle 3/10 - Loss: 2.3026 - Accuracy: 0.1000\n",
            "Cycle 4/10 - Loss: 2.3022 - Accuracy: 0.1087\n",
            "Cycle 5/10 - Loss: 2.3019 - Accuracy: 0.1686\n",
            "Cycle 6/10 - Loss: 2.3017 - Accuracy: 0.1777\n",
            "Cycle 7/10 - Loss: 2.3014 - Accuracy: 0.1364\n",
            "Cycle 8/10 - Loss: 2.3011 - Accuracy: 0.1093\n",
            "Cycle 9/10 - Loss: 2.3009 - Accuracy: 0.1021\n",
            "Cycle 10/10 - Loss: 2.3006 - Accuracy: 0.1006\n",
            "Final Accuracy with sgd: 0.1001\n",
            "\n",
            "Training using adam optimizer with 128 hidden nodes and LR=0.001\n",
            "Cycle 1/10 - Loss: 2.3041 - Accuracy: 0.1000\n",
            "Cycle 2/10 - Loss: 2.2721 - Accuracy: 0.1899\n",
            "Cycle 3/10 - Loss: 2.2450 - Accuracy: 0.2841\n",
            "Cycle 4/10 - Loss: 2.2208 - Accuracy: 0.3347\n",
            "Cycle 5/10 - Loss: 2.1990 - Accuracy: 0.4001\n",
            "Cycle 6/10 - Loss: 2.1797 - Accuracy: 0.4489\n",
            "Cycle 7/10 - Loss: 2.1627 - Accuracy: 0.4783\n",
            "Cycle 8/10 - Loss: 2.1480 - Accuracy: 0.5056\n",
            "Cycle 9/10 - Loss: 2.1350 - Accuracy: 0.5437\n",
            "Cycle 10/10 - Loss: 2.1236 - Accuracy: 0.5756\n",
            "Final Accuracy with adam: 0.6043\n",
            "\n",
            "Training using momentum optimizer with 100 hidden nodes and LR=0.01\n",
            "Cycle 1/10 - Loss: 2.3032 - Accuracy: 0.1000\n",
            "Cycle 2/10 - Loss: 2.3031 - Accuracy: 0.1000\n",
            "Cycle 3/10 - Loss: 2.3030 - Accuracy: 0.1000\n",
            "Cycle 4/10 - Loss: 2.3029 - Accuracy: 0.1000\n",
            "Cycle 5/10 - Loss: 2.3027 - Accuracy: 0.1000\n",
            "Cycle 6/10 - Loss: 2.3025 - Accuracy: 0.1000\n",
            "Cycle 7/10 - Loss: 2.3023 - Accuracy: 0.1000\n",
            "Cycle 8/10 - Loss: 2.3021 - Accuracy: 0.1003\n",
            "Cycle 9/10 - Loss: 2.3019 - Accuracy: 0.1070\n",
            "Cycle 10/10 - Loss: 2.3016 - Accuracy: 0.1506\n",
            "Final Accuracy with momentum: 0.1969\n",
            "\n",
            "  Optimizer  Learning Rate  Hidden Layer Size  Final Accuracy\n",
            "0       sgd          0.100                 64        0.100143\n",
            "1      adam          0.001                128        0.604286\n",
            "2  momentum          0.010                100        0.196857\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import cifar10, cifar100\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values between 0 and 1\n",
        "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Define neural network parameters\n",
        "input_nodes = 32 * 32 * 3  # CIFAR images are 32x32x3 (RGB)\n",
        "hidden_nodes = 128  # Hidden layer size\n",
        "output_nodes = 10  # 10 classes in CIFAR-10\n",
        "learning_rate = 0.001\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "\n",
        "# Initialize weights\n",
        "np.random.seed(42)\n",
        "W1 = np.random.randn(input_nodes, hidden_nodes) * 0.01\n",
        "b1 = np.zeros((1, hidden_nodes))\n",
        "W2 = np.random.randn(hidden_nodes, output_nodes) * 0.01\n",
        "b2 = np.zeros((1, output_nodes))\n",
        "\n",
        "def activation_relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def activation_softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "def loss_cross_entropy(predictions, actual):\n",
        "    return -np.mean(np.sum(actual * np.log(predictions + 1e-9), axis=1))\n",
        "\n",
        "def evaluate_accuracy(predictions, actual):\n",
        "    return np.mean(np.argmax(predictions, axis=1) == np.argmax(actual, axis=1))\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    # Forward propagation\n",
        "    X_train_flat = X_train.reshape(len(X_train), -1)\n",
        "    layer1_output = np.dot(X_train_flat, W1) + b1\n",
        "    activated_layer1 = activation_relu(layer1_output)\n",
        "    layer2_output = np.dot(activated_layer1, W2) + b2\n",
        "    predictions = activation_softmax(layer2_output)\n",
        "\n",
        "    # Compute loss and accuracy\n",
        "    loss_value = loss_cross_entropy(predictions, y_train)\n",
        "    accuracy_value = evaluate_accuracy(predictions, y_train)\n",
        "\n",
        "    # Backpropagation\n",
        "    error_output = predictions - y_train\n",
        "    gradient_W2 = np.dot(activated_layer1.T, error_output) / len(X_train)\n",
        "    gradient_b2 = np.sum(error_output, axis=0, keepdims=True) / len(X_train)\n",
        "    error_hidden = np.dot(error_output, W2.T) * (activated_layer1 > 0)\n",
        "    gradient_W1 = np.dot(X_train_flat.T, error_hidden) / len(X_train)\n",
        "    gradient_b1 = np.sum(error_hidden, axis=0, keepdims=True) / len(X_train)\n",
        "\n",
        "    # Update weights using SGD\n",
        "    W1 -= learning_rate * gradient_W1\n",
        "    b1 -= learning_rate * gradient_b1\n",
        "    W2 -= learning_rate * gradient_W2\n",
        "    b2 -= learning_rate * gradient_b2\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss_value:.4f} - Accuracy: {accuracy_value:.4f}\")\n",
        "\n",
        "# Evaluate on test data\n",
        "X_test_flat = X_test.reshape(len(X_test), -1)\n",
        "test_layer1 = np.dot(X_test_flat, W1) + b1\n",
        "test_activated1 = activation_relu(test_layer1)\n",
        "test_layer2 = np.dot(test_activated1, W2) + b2\n",
        "test_predictions = activation_softmax(test_layer2)\n",
        "final_accuracy = evaluate_accuracy(test_predictions, y_test)\n",
        "print(f\"Final Accuracy on CIFAR-10: {final_accuracy:.4f}\")\n",
        "\n",
        "# CIFAR-100 Implementation\n",
        "(X_train100, y_train100), (X_test100, y_test100) = cifar100.load_data()\n",
        "X_train100, X_test100 = X_train100 / 255.0, X_test100 / 255.0\n",
        "y_train100 = to_categorical(y_train100, 100)\n",
        "y_test100 = to_categorical(y_test100, 100)\n",
        "\n",
        "# Modify output layer for CIFAR-100\n",
        "output_nodes = 100\n",
        "W2 = np.random.randn(hidden_nodes, output_nodes) * 0.01\n",
        "b2 = np.zeros((1, output_nodes))\n",
        "\n",
        "# Training loop for CIFAR-100\n",
        "for epoch in range(epochs):\n",
        "    X_train100_flat = X_train100.reshape(len(X_train100), -1)\n",
        "    layer1_output = np.dot(X_train100_flat, W1) + b1\n",
        "    activated_layer1 = activation_relu(layer1_output)\n",
        "    layer2_output = np.dot(activated_layer1, W2) + b2\n",
        "    predictions = activation_softmax(layer2_output)\n",
        "\n",
        "    loss_value = loss_cross_entropy(predictions, y_train100)\n",
        "    accuracy_value = evaluate_accuracy(predictions, y_train100)\n",
        "\n",
        "    error_output = predictions - y_train100\n",
        "    gradient_W2 = np.dot(activated_layer1.T, error_output) / len(X_train100)\n",
        "    gradient_b2 = np.sum(error_output, axis=0, keepdims=True) / len(X_train100)\n",
        "    error_hidden = np.dot(error_output, W2.T) * (activated_layer1 > 0)\n",
        "    gradient_W1 = np.dot(X_train100_flat.T, error_hidden) / len(X_train100)\n",
        "    gradient_b1 = np.sum(error_hidden, axis=0, keepdims=True) / len(X_train100)\n",
        "\n",
        "    W1 -= learning_rate * gradient_W1\n",
        "    b1 -= learning_rate * gradient_b1\n",
        "    W2 -= learning_rate * gradient_W2\n",
        "    b2 -= learning_rate * gradient_b2\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss_value:.4f} - Accuracy: {accuracy_value:.4f}\")\n",
        "\n",
        "# Evaluate on CIFAR-100\n",
        "test_layer1 = np.dot(X_test100.reshape(len(X_test100), -1), W1) + b1\n",
        "test_activated1 = activation_relu(test_layer1)\n",
        "test_layer2 = np.dot(test_activated1, W2) + b2\n",
        "test_predictions = activation_softmax(test_layer2)\n",
        "final_accuracy = evaluate_accuracy(test_predictions, y_test100)\n",
        "print(f\"Final Accuracy on CIFAR-100: {final_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpr6ydiM41iD",
        "outputId": "99251c40-cabc-4e39-ddd6-d4234179a6c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 0us/step\n",
            "Epoch 1/10 - Loss: 2.3022 - Accuracy: 0.0952\n",
            "Epoch 2/10 - Loss: 2.3022 - Accuracy: 0.0952\n",
            "Epoch 3/10 - Loss: 2.3022 - Accuracy: 0.0951\n",
            "Epoch 4/10 - Loss: 2.3022 - Accuracy: 0.0952\n",
            "Epoch 5/10 - Loss: 2.3022 - Accuracy: 0.0953\n",
            "Epoch 6/10 - Loss: 2.3021 - Accuracy: 0.0954\n",
            "Epoch 7/10 - Loss: 2.3021 - Accuracy: 0.0955\n",
            "Epoch 8/10 - Loss: 2.3021 - Accuracy: 0.0956\n",
            "Epoch 9/10 - Loss: 2.3021 - Accuracy: 0.0956\n",
            "Epoch 10/10 - Loss: 2.3021 - Accuracy: 0.0958\n",
            "Final Accuracy on CIFAR-10: 0.0932\n",
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "\u001b[1m169001437/169001437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 0us/step\n",
            "Epoch 1/10 - Loss: 4.6057 - Accuracy: 0.0103\n",
            "Epoch 2/10 - Loss: 4.6057 - Accuracy: 0.0103\n",
            "Epoch 3/10 - Loss: 4.6057 - Accuracy: 0.0104\n",
            "Epoch 4/10 - Loss: 4.6057 - Accuracy: 0.0104\n",
            "Epoch 5/10 - Loss: 4.6057 - Accuracy: 0.0104\n",
            "Epoch 6/10 - Loss: 4.6057 - Accuracy: 0.0104\n",
            "Epoch 7/10 - Loss: 4.6057 - Accuracy: 0.0104\n",
            "Epoch 8/10 - Loss: 4.6057 - Accuracy: 0.0105\n",
            "Epoch 9/10 - Loss: 4.6056 - Accuracy: 0.0104\n",
            "Epoch 10/10 - Loss: 4.6056 - Accuracy: 0.0104\n",
            "Final Accuracy on CIFAR-100: 0.0112\n"
          ]
        }
      ]
    }
  ]
}